{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "import es_dep_news_trf\n",
    "import pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4786ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util as imp\n",
    "import sys\n",
    "\n",
    "spec = imp.spec_from_file_location(\n",
    "    'twitter_connection', \n",
    "    '../twitter-connection/__init__.py')\n",
    "twit = imp.module_from_spec(spec)\n",
    "sys.modules[spec.name] = twit\n",
    "spec.loader.exec_module(twit)\n",
    "\n",
    "from twitter_connection import connection as tc\n",
    "from twitter_connection import response as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5355260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File path to the bearer token. Requires a prefix to identify the\n",
    "  token, which is just 'PERSONAL $BEARER_TOKEN$' by default -- \n",
    "  can be specified upon initialization of TwitterConnection\n",
    "'''\n",
    "cred_path = r'../twitter-connection/credentials.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0785769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Portuguese-only verbs\n",
    "pt_verbs = {'dizer', 'supor', 'duvidar', 'acreditar', 'achar', 'lembrar', 'recear', 'predizer', 'adivinhar', 'conjeturar', 'chutar', 'dar(se) conta', 'desejar', 'oxala', 'tomara'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f27afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../extraction/verb-stem-clean.txt') as f:\n",
    "    verb_stem = json.load(f)\n",
    "    \n",
    "verbs_volit = {vs[0]: vs[1] for vs in list(verb_stem.items())[len(verb_stem)-17:]}\n",
    "es_verbs_volit = {v: verbs_volit[v] for v in (verbs_volit.keys() - pt_verbs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query_cond = 'lang:es has:geo -is:retweet -has:links '\n",
    "pt_query_cond = 'lang:pt has:geo -is:retweet -has:links '\n",
    "fields_tweet = 'tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets'\n",
    "fields_expan = 'expansions=author_id,geo.place_id,entities.mentions.username'\n",
    "fields_user = 'user.fields=created_at,location,public_metrics'\n",
    "fields_place = 'place.fields=country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18b0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF')\n",
    "\n",
    "# pt_conn = tc.TwitterConnection(\n",
    "#     is_archive=True,\n",
    "#     cred_prefix='PROF') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbb6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn.set_query(conditions=es_query_cond)\n",
    "es_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)\n",
    "\n",
    "# pt_conn.set_query(conditions=pt_query_cond)\n",
    "# pt_conn.set_fields(tweet=fields_tweet, \n",
    "#                       expansions=fields_expan, \n",
    "#                       user=fields_user,\n",
    "#                       place=fields_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993c4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.now().strftime('%d%m%Y-at-%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4df0aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(\"supl\" OR \"supli\") lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "3\n",
      "test/es/05072021-at-1711/es-SUPLICAR-Tweet-3.csv\n",
      "5\n",
      "test/es/05072021-at-1711/es-SUPLICAR-Users-5.csv\n",
      "3\n",
      "test/es/05072021-at-1711/es-SUPLICAR-Places-3.csv\n"
     ]
    }
   ],
   "source": [
    "v = 'suplicar'\n",
    "s = '\"supl\" OR \"supli\"'\n",
    "\n",
    "test_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF')\n",
    "\n",
    "test_conn.set_query(conditions=es_query_cond)\n",
    "test_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)\n",
    "\n",
    "test_conn.connect('(' + s +')', is_next=True)\n",
    "print(test_conn.url)\n",
    "\n",
    "res = tr.Response(test_conn.response)\n",
    "res.to_csv(\n",
    "    lang='es', time=time, verb=v, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4927d24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving word: oxala\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(oxala) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "11\n",
      "test/es/05072021-at-1656/es-OXALA-Tweet-11.csv\n",
      "13\n",
      "test/es/05072021-at-1656/es-OXALA-Users-13.csv\n",
      "6\n",
      "test/es/05072021-at-1656/es-OXALA-Places-6.csv\n",
      "Retrieving word: rogar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(rueg OR rog) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "13\n",
      "test/es/05072021-at-1656/es-ROGAR-Tweet-13.csv\n",
      "63\n",
      "test/es/05072021-at-1656/es-ROGAR-Users-63.csv\n",
      "11\n",
      "test/es/05072021-at-1656/es-ROGAR-Places-11.csv\n",
      "Retrieving word: insistir\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(insist) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "1\n",
      "test/es/05072021-at-1656/es-INSISTIR-Tweet-1.csv\n",
      "3\n",
      "test/es/05072021-at-1656/es-INSISTIR-Users-3.csv\n",
      "1\n",
      "test/es/05072021-at-1656/es-INSISTIR-Places-1.csv\n",
      "Retrieving word: querer\n",
      "https://api.twitter.com/2/tweets/search/all?query=(quer OR quier OR quis) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 96\n",
      "After append: 195\n",
      "No next token! ('next_token',)\n",
      "Before append: 195\n",
      "After append: 282\n",
      "282\n",
      "test/es/05072021-at-1656/es-QUERER-Tweet-282.csv\n",
      "595\n",
      "test/es/05072021-at-1656/es-QUERER-Users-595.csv\n",
      "195\n",
      "test/es/05072021-at-1656/es-QUERER-Places-195.csv\n",
      "Exception while saving to CSV! >>>\n",
      " [21, 'Is a directory']\n",
      "<<<\n",
      "Retrieving word: ojala\n",
      "https://api.twitter.com/2/tweets/search/all?query=(ojala OR ohala) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "Before append: 200\n",
      "After append: 300\n",
      "Before append: 300\n",
      "After append: 400\n",
      "400\n",
      "test/es/05072021-at-1656/es-OJALA-Tweet-400.csv\n",
      "839\n",
      "test/es/05072021-at-1656/es-OJALA-Users-839.csv\n",
      "308\n",
      "test/es/05072021-at-1656/es-OJALA-Places-308.csv\n",
      "Retrieving word: suplicar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(suplic OR supliq) lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdj7j10cvfq9tpjco9tvlg1od52m5&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "1\n",
      "test/es/05072021-at-1656/es-SUPLICAR-Tweet-1.csv\n",
      "3\n",
      "test/es/05072021-at-1656/es-SUPLICAR-Users-3.csv\n",
      "1\n",
      "test/es/05072021-at-1656/es-SUPLICAR-Places-1.csv\n",
      "Retrieving word: mandar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(mand) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "7\n",
      "test/es/05072021-at-1656/es-MANDAR-Tweet-7.csv\n",
      "9\n",
      "test/es/05072021-at-1656/es-MANDAR-Users-9.csv\n",
      "4\n",
      "test/es/05072021-at-1656/es-MANDAR-Places-4.csv\n",
      "Retrieving word: sugerir\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(sugier OR suger OR sugir) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "2\n",
      "test/es/05072021-at-1656/es-SUGERIR-Tweet-2.csv\n",
      "5\n",
      "test/es/05072021-at-1656/es-SUGERIR-Users-5.csv\n",
      "1\n",
      "test/es/05072021-at-1656/es-SUGERIR-Places-1.csv\n",
      "Retrieving word: alegrar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(alegr) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "3\n",
      "test/es/05072021-at-1656/es-ALEGRAR-Tweet-3.csv\n",
      "2\n",
      "test/es/05072021-at-1656/es-ALEGRAR-Users-2.csv\n",
      "2\n",
      "test/es/05072021-at-1656/es-ALEGRAR-Places-2.csv\n",
      "Retrieving word: preocupar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(preocup) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "3\n",
      "test/es/05072021-at-1656/es-PREOCUPAR-Tweet-3.csv\n",
      "7\n",
      "test/es/05072021-at-1656/es-PREOCUPAR-Users-7.csv\n",
      "3\n",
      "test/es/05072021-at-1656/es-PREOCUPAR-Places-3.csv\n",
      "Retrieving word: esperar\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(esper) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "28\n",
      "test/es/05072021-at-1656/es-ESPERAR-Tweet-28.csv\n",
      "75\n",
      "test/es/05072021-at-1656/es-ESPERAR-Users-75.csv\n",
      "24\n",
      "test/es/05072021-at-1656/es-ESPERAR-Places-24.csv\n"
     ]
    }
   ],
   "source": [
    "for vs in es_v.items():\n",
    "    if vs[1]==0:\n",
    "        continue\n",
    "    \n",
    "    verb = vs[0]\n",
    "    stems = '(' + vs[1] + ')'\n",
    "    print(f'Retrieving word: {verb}')\n",
    "    \n",
    "    response = tr.Response()\n",
    "    \n",
    "    while True:\n",
    "        es_conn.connect(stems, is_next=True, time_interval=1)\n",
    "        \n",
    "        if len(response.schema)==0:\n",
    "            print(es_conn.url)\n",
    "        \n",
    "        new = tr.Response(es_conn.response)\n",
    "        \n",
    "#         # Remove '@...' mentions\n",
    "#         new.schema['data'].loc[:, 'text'] = new.schema['data'].loc[:, 'text']\\\n",
    "#             .str.replace(r'(@[\\w]+ )', '', regex=True)\\\n",
    "#             .apply(unidecode)\n",
    "        \n",
    "#         text_analyzed = analyze(new.schema['data'].loc[:, ['id', 'text']], 'es')\n",
    "        \n",
    "#         # Entries without desired verb\n",
    "#         no_verb = ~(text_analyzed.loc[:, 'lemma'].str.contains(verb))\n",
    "#         print(f'Found {no_verb.sum()} without \"{verb}\"')\n",
    "        \n",
    "#         new.join(to='data', data=text_analyzed, on='id')\n",
    "        \n",
    "#         new.schema['data'].drop(new.schema['data'].loc[no_verb, :].index, inplace=True)\n",
    "#         response.reset_index()       \n",
    "            \n",
    "        response.append(new)\n",
    "        \n",
    "        if (response.schema['data'].original.shape[0] >= 400) or (not es_conn.has_next):\n",
    "            break\n",
    "    \n",
    "    response.to_csv(\n",
    "        lang='es', time=time, verb=verb, is_test=True)\n",
    "    es_verbs[verb] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tr.Response(tr.retrieve(f'es-{s}-test-data.txt'))\n",
    "# pt = tr.Response(tr.retrieve(f'pt-{s}-test-data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c60c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(es.schema['data'].head(3))\n",
    "# display(pt.schema['data'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09866bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ae790",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_es = es_dep_news_trf.load()\n",
    "# nlp_pt = pt_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(tokenized):\n",
    "    return ' '.join([f'{t.text}-({t.pos_})' for t in tokenized if ((t.pos_!='PUNCT') and (t.pos_!='SPACE'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokenized):\n",
    "    return ' '.join([t.lemma_ for t in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text, lang):\n",
    "    # Tokenized\n",
    "    text_nlp = text.loc[:, 'text'].apply(nlp_es if lang=='es' else nlp_pt)\n",
    "    \n",
    "    pos = text_nlp.apply(get_pos_tags).rename('pos')\n",
    "    lemma = text_nlp.apply(lemmatize).rename('lemma')\n",
    "    \n",
    "    return pd.concat([text.loc[:, 'id'], pos, lemma], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = 'decir'\n",
    "stems = '(dig OR dec OR dij OR dir OR dic)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c67586",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tr.Response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86255bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tr.Response(tr.retrieve(f'es-{s}-test-data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '@...' mentions\n",
    "es.schema['data'].loc[:, 'text'] = es.schema['data'].loc[:, 'text']\\\n",
    "    .str.replace(r'(@[\\w]+ )', '', regex=True)\\\n",
    "    .apply(unidecode)\n",
    "\n",
    "text_analyzed = analyze(es.schema['data'].loc[:, ['id', 'text']], 'es')\n",
    "\n",
    "display(text_analyzed.head())\n",
    "\n",
    "# Entries without desired verb\n",
    "no_verb = ~(text_analyzed.loc[:, 'lemma'].str.contains(verb))\n",
    "print(f'Found {no_verb.sum()} without \"{verb}\"')\n",
    "\n",
    "display(pd.concat([es.schema['data'].loc[:, 'text'], es.schema['data'].loc[no_verb, :]], axis=1))\n",
    "\n",
    "es.join(to='data', data=text_analyzed, on='id')\n",
    "\n",
    "es.schema['data'].drop(es.schema['data'].loc[no_verb, :].index, inplace=True)\n",
    "response.reset_index()       \n",
    "    \n",
    "response.append(es)\n",
    "\n",
    "response.to_csv('es', verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "es_t = es_text.apply(unidecode)\n",
    "pt_t = pt_text.apply(unidecode)\n",
    "\n",
    "es_bad = is_bad_verb('es', es_t, 'vi OR ve OR ve')\n",
    "pt_bad = is_bad_verb('pt', pt_t, 'vi OR ve OR ve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_out = pd.concat(\n",
    "    [es.schema['data'].loc[:, 'text'], es_bad], axis=1)\n",
    "pt_out = pd.concat(\n",
    "    [pt.schema['data'].loc[:, 'text'], pt_bad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('es', 'ver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac145c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_bad.loc[:, 'is_duplicate'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(es_out.head())\n",
    "display(pt_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bdbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = es_out.loc[~es_bad.loc[:, 'has_verb'], ['text', 'lemmad']]\n",
    "\n",
    "for i in range(b.shape[0]):\n",
    "    print(f'CASE:\\n')\n",
    "    print(f'ORIGINAL:\\n')\n",
    "    print(b.iloc[i, 0])\n",
    "    print(f'LEMMAD:\\n')\n",
    "    print(b.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f53451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
