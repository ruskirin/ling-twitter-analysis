{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from time import sleep\n",
    "import es_dep_news_trf\n",
    "import pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4786ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util as imp\n",
    "import sys\n",
    "\n",
    "spec = imp.spec_from_file_location(\n",
    "    'twitter_connection', \n",
    "    '../twitter-connection/__init__.py')\n",
    "twit = imp.module_from_spec(spec)\n",
    "sys.modules[spec.name] = twit\n",
    "spec.loader.exec_module(twit)\n",
    "\n",
    "from twitter_connection import connection as tc\n",
    "from twitter_connection import response as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5355260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File path to the bearer token. Requires a prefix to identify the\n",
    "  token, which is just 'PERSONAL $BEARER_TOKEN$' by default -- \n",
    "  can be specified upon initialization of TwitterConnection\n",
    "'''\n",
    "cred_path = r'../twitter-connection/credentials.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e90564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Portuguese-only verbs\n",
    "pt_verbs = {'dizer', 'supor', 'duvidar', 'acreditar', 'achar', 'lembrar', 'recear', 'predizer', 'adivinhar', 'conjeturar', 'chutar', 'dar(se) conta', 'desejar', 'oxala', 'tomara'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../extraction/verb-stem-clean.txt') as f:\n",
    "    verb_stem = json.load(f)\n",
    "    \n",
    "verbs_volit = [v for v in list(verb_stem.keys())[len(verb_stem)-17:]]\n",
    "es_verbs_volit = [v for v in (set(verbs_volit) - pt_verbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query_cond = 'lang:es has:geo -is:retweet -has:links '\n",
    "pt_query_cond = 'lang:pt has:geo -is:retweet -has:links '\n",
    "fields_tweet = 'tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets'\n",
    "fields_expan = 'expansions=author_id,geo.place_id,entities.mentions.username'\n",
    "fields_user = 'user.fields=created_at,location,public_metrics'\n",
    "fields_place = 'place.fields=country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18b0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF')\n",
    "\n",
    "# pt_conn = tc.TwitterConnection(\n",
    "#     is_archive=True,\n",
    "#     cred_prefix='PROF') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbb6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn.set_query(conditions=es_query_cond)\n",
    "es_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)\n",
    "\n",
    "# pt_conn.set_query(conditions=pt_query_cond)\n",
    "# pt_conn.set_fields(tweet=fields_tweet, \n",
    "#                       expansions=fields_expan, \n",
    "#                       user=fields_user,\n",
    "#                       place=fields_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e92f02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dt.now().strftime('%d%m%Y-at-%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5be4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Jorge Ramos', 'Cárdenas Palomino', '\"harry styles\"', 'Richard Donner', 'Generales', 'olivia', 'Criminales', 'Sofia', 'Alba', '\"La Posta\"', 'Peru', 'Capital', 'Neymar', 'Superman', 'Gallese']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b442d2c",
   "metadata": {},
   "source": [
    "v = 'suplicar'\n",
    "s = '\"supl\" OR \"supli\"'\n",
    "\n",
    "test_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF')\n",
    "\n",
    "test_conn.set_query(conditions=es_query_cond)\n",
    "test_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)\n",
    "\n",
    "test_conn.connect('(' + s +')', is_next=True)\n",
    "print(test_conn.url)\n",
    "\n",
    "res = tr.Response(test_conn.response)\n",
    "res.to_csv(\n",
    "    lang='es', time=time, verb=v, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b13f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4927d24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving topic: Jorge Ramos\n",
      "https://api.twitter.com/2/tweets/search/all?query=Jorge Ramos lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "No next token! ('next_token',)\n",
      "Before append: 100\n",
      "After append: 166\n",
      "166\n",
      "test/es/05072021-at-2103/es-JORGE RAMOS-original-tweets-166-0.csv\n",
      "287\n",
      "test/es/05072021-at-2103/es-JORGE RAMOS-original-users-287-0.csv\n",
      "117\n",
      "test/es/05072021-at-2103/es-JORGE RAMOS-original-places-117-0.csv\n",
      "Retrieving topic: Cárdenas Palomino\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=Cárdenas Palomino lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "31\n",
      "test/es/05072021-at-2103/es-CÁRDENAS PALOMINO-original-tweets-31-0.csv\n",
      "63\n",
      "test/es/05072021-at-2103/es-CÁRDENAS PALOMINO-original-users-63-0.csv\n",
      "22\n",
      "test/es/05072021-at-2103/es-CÁRDENAS PALOMINO-original-places-22-0.csv\n",
      "Retrieving topic: \"harry styles\"\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=\"harry styles\" lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "87\n",
      "test/es/05072021-at-2103/es-\"HARRY STYLES\"-original-tweets-87-0.csv\n",
      "131\n",
      "test/es/05072021-at-2103/es-\"HARRY STYLES\"-original-users-131-0.csv\n",
      "64\n",
      "test/es/05072021-at-2103/es-\"HARRY STYLES\"-original-places-64-0.csv\n",
      "Retrieving topic: Richard Donner\n",
      "No next token! ('next_token',)\n",
      "https://api.twitter.com/2/tweets/search/all?query=Richard Donner lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "20\n",
      "test/es/05072021-at-2103/es-RICHARD DONNER-original-tweets-20-0.csv\n",
      "28\n",
      "test/es/05072021-at-2103/es-RICHARD DONNER-original-users-28-0.csv\n",
      "15\n",
      "test/es/05072021-at-2103/es-RICHARD DONNER-original-places-15-0.csv\n",
      "Retrieving topic: Generales\n",
      "https://api.twitter.com/2/tweets/search/all?query=Generales lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 199\n",
      "Before append: 199\n",
      "After append: 299\n",
      "Before append: 299\n",
      "After append: 399\n",
      "Before append: 399\n",
      "After append: 498\n",
      "Before append: 498\n",
      "After append: 598\n",
      "598\n",
      "test/es/05072021-at-2103/es-GENERALES-original-tweets-598-0.csv\n",
      "1428\n",
      "test/es/05072021-at-2103/es-GENERALES-original-users-1428-0.csv\n",
      "440\n",
      "test/es/05072021-at-2103/es-GENERALES-original-places-440-0.csv\n",
      "Retrieving topic: olivia\n",
      "https://api.twitter.com/2/tweets/search/all?query=olivia lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdg86pis4wbucpzfc86g8dfgo0z25&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 98\n",
      "After append: 198\n",
      "Before append: 198\n",
      "After append: 298\n",
      "No next token! ('next_token',)\n",
      "Before append: 298\n",
      "After append: 359\n",
      "359\n",
      "test/es/05072021-at-2103/es-OLIVIA-original-tweets-359-0.csv\n",
      "475\n",
      "test/es/05072021-at-2103/es-OLIVIA-original-users-475-0.csv\n",
      "270\n",
      "test/es/05072021-at-2103/es-OLIVIA-original-places-270-0.csv\n",
      "Retrieving topic: Criminales\n",
      "https://api.twitter.com/2/tweets/search/all?query=Criminales lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 99\n",
      "After append: 199\n",
      "Before append: 199\n",
      "After append: 298\n",
      "Before append: 298\n",
      "After append: 398\n",
      "Before append: 398\n",
      "After append: 498\n",
      "Before append: 498\n",
      "After append: 598\n",
      "598\n",
      "test/es/05072021-at-2103/es-CRIMINALES-original-tweets-598-0.csv\n",
      "1364\n",
      "test/es/05072021-at-2103/es-CRIMINALES-original-users-1364-0.csv\n",
      "348\n",
      "test/es/05072021-at-2103/es-CRIMINALES-original-places-348-0.csv\n",
      "Retrieving topic: Sofia\n",
      "https://api.twitter.com/2/tweets/search/all?query=Sofia lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdgapcjpiy60bk2r2jkbbc5s81abh&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 99\n",
      "After append: 199\n",
      "Before append: 199\n",
      "After append: 299\n",
      "Before append: 299\n",
      "After append: 398\n",
      "Before append: 398\n",
      "After append: 495\n",
      "Before append: 495\n",
      "After append: 595\n",
      "595\n",
      "test/es/05072021-at-2103/es-SOFIA-original-tweets-595-0.csv\n",
      "1146\n",
      "test/es/05072021-at-2103/es-SOFIA-original-users-1146-0.csv\n",
      "433\n",
      "test/es/05072021-at-2103/es-SOFIA-original-places-433-0.csv\n",
      "Retrieving topic: Alba\n",
      "https://api.twitter.com/2/tweets/search/all?query=Alba lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdg86cqqaeteibebwhlc4w0o6nyil&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 99\n",
      "After append: 199\n",
      "No next token! ('next_token',)\n",
      "Before append: 199\n",
      "After append: 230\n",
      "230\n",
      "test/es/05072021-at-2103/es-ALBA-original-tweets-230-0.csv\n",
      "516\n",
      "test/es/05072021-at-2103/es-ALBA-original-users-516-0.csv\n",
      "166\n",
      "test/es/05072021-at-2103/es-ALBA-original-places-166-0.csv\n",
      "Retrieving topic: \"La Posta\"\n",
      "https://api.twitter.com/2/tweets/search/all?query=\"La Posta\" lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "Before append: 200\n",
      "After append: 300\n",
      "No next token! ('next_token',)\n",
      "Before append: 300\n",
      "After append: 370\n",
      "370\n",
      "test/es/05072021-at-2103/es-\"LA POSTA\"-original-tweets-370-0.csv\n",
      "698\n",
      "test/es/05072021-at-2103/es-\"LA POSTA\"-original-users-698-0.csv\n",
      "199\n",
      "test/es/05072021-at-2103/es-\"LA POSTA\"-original-places-199-0.csv\n",
      "Retrieving topic: Peru\n",
      "https://api.twitter.com/2/tweets/search/all?query=Peru lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "Before append: 200\n",
      "After append: 300\n",
      "Before append: 300\n",
      "After append: 400\n",
      "Before append: 400\n",
      "After append: 500\n",
      "500\n",
      "test/es/05072021-at-2103/es-PERU-original-tweets-500-0.csv\n",
      "634\n",
      "test/es/05072021-at-2103/es-PERU-original-users-634-0.csv\n",
      "372\n",
      "test/es/05072021-at-2103/es-PERU-original-places-372-0.csv\n",
      "Retrieving topic: Capital\n",
      "https://api.twitter.com/2/tweets/search/all?query=Capital lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdj7j36l1yvnqmop4zah2jd4n7vct&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "Before append: 200\n",
      "After append: 298\n",
      "Before append: 298\n",
      "After append: 398\n",
      "Before append: 398\n",
      "After append: 498\n",
      "Before append: 498\n",
      "After append: 598\n",
      "598\n",
      "test/es/05072021-at-2103/es-CAPITAL-original-tweets-598-0.csv\n",
      "1511\n",
      "test/es/05072021-at-2103/es-CAPITAL-original-users-1511-0.csv\n",
      "445\n",
      "test/es/05072021-at-2103/es-CAPITAL-original-places-445-0.csv\n",
      "Retrieving topic: Neymar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.twitter.com/2/tweets/search/all?query=Neymar lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdj6ou1mmm613b27lvmeddjf2w6t9&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "Before append: 200\n",
      "After append: 300\n",
      "Before append: 300\n",
      "After append: 400\n",
      "Before append: 400\n",
      "After append: 500\n",
      "500\n",
      "test/es/05072021-at-2103/es-NEYMAR-original-tweets-500-0.csv\n",
      "709\n",
      "test/es/05072021-at-2103/es-NEYMAR-original-users-709-0.csv\n",
      "326\n",
      "test/es/05072021-at-2103/es-NEYMAR-original-places-326-0.csv\n",
      "Retrieving topic: Superman\n",
      "https://api.twitter.com/2/tweets/search/all?query=Superman lang:es has:geo -is:retweet -has:links &max_results=100&next_token=b26v89c19zqg8o3fpdg9gamd0nluoyp5oaz86c85oogot&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "No next token! ('next_token',)\n",
      "Before append: 200\n",
      "After append: 239\n",
      "239\n",
      "test/es/05072021-at-2103/es-SUPERMAN-original-tweets-239-0.csv\n",
      "470\n",
      "test/es/05072021-at-2103/es-SUPERMAN-original-users-470-0.csv\n",
      "194\n",
      "test/es/05072021-at-2103/es-SUPERMAN-original-places-194-0.csv\n",
      "Retrieving topic: Gallese\n",
      "https://api.twitter.com/2/tweets/search/all?query=Gallese lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Before append: 100\n",
      "After append: 200\n",
      "No next token! ('next_token',)\n",
      "Before append: 200\n",
      "After append: 269\n",
      "269\n",
      "test/es/05072021-at-2103/es-GALLESE-original-tweets-269-0.csv\n",
      "295\n",
      "test/es/05072021-at-2103/es-GALLESE-original-users-295-0.csv\n",
      "200\n",
      "test/es/05072021-at-2103/es-GALLESE-original-places-200-0.csv\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    if topic in finished:\n",
    "        continue\n",
    "    \n",
    "    print(f'Retrieving topic: {topic}')\n",
    "    \n",
    "    response = tr.Response(lang='es', topic=topic)\n",
    "    \n",
    "    while True:\n",
    "        es_conn.connect(topic, is_next=True, time_interval=1)\n",
    "        \n",
    "        if len(response.schema)==0:\n",
    "            print(es_conn.url)\n",
    "        \n",
    "        new = tr.Response(lang='es', \n",
    "                          topic=topic, \n",
    "                          response=es_conn.response)\n",
    "        \n",
    "#         text_analyzed = analyze(new.schema['data'].loc[:, ['id', 'text']], 'es')\n",
    "        \n",
    "#         # Entries without desired verb\n",
    "#         no_verb = ~(text_analyzed.loc[:, 'lemma'].str.contains(verb))\n",
    "#         print(f'Found {no_verb.sum()} without \"{verb}\"')\n",
    "        \n",
    "#         new.join(to='data', data=text_analyzed, on='id')\n",
    "        \n",
    "#         new.schema['data'].drop(new.schema['data'].loc[no_verb, :].index, inplace=True)\n",
    "#         response.reset_index()       \n",
    "            \n",
    "        response.append(new)\n",
    "        \n",
    "        if (response.schema['data'].original.shape[0] >= 500) or (not es_conn.has_next):\n",
    "            break\n",
    "    \n",
    "    response.save_csv(time=time, is_test=True)\n",
    "    finished.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137da6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tr.Response(tr.retrieve(f'es-{s}-test-data.txt'))\n",
    "# pt = tr.Response(tr.retrieve(f'pt-{s}-test-data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c60c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(es.schema['data'].head(3))\n",
    "# display(pt.schema['data'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09866bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ae790",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_es = es_dep_news_trf.load()\n",
    "# nlp_pt = pt_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(tokenized):\n",
    "    return ' '.join([f'{t.text}-({t.pos_})' for t in tokenized if ((t.pos_!='PUNCT') and (t.pos_!='SPACE'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokenized):\n",
    "    return ' '.join([t.lemma_ for t in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text, lang):\n",
    "    # Tokenized\n",
    "    text_nlp = text.loc[:, 'text'].apply(nlp_es if lang=='es' else nlp_pt)\n",
    "    \n",
    "    pos = text_nlp.apply(get_pos_tags).rename('pos')\n",
    "    lemma = text_nlp.apply(lemmatize).rename('lemma')\n",
    "    \n",
    "    return pd.concat([text.loc[:, 'id'], pos, lemma], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = 'decir'\n",
    "stems = '(dig OR dec OR dij OR dir OR dic)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c67586",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tr.Response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86255bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tr.Response(tr.retrieve(f'es-{s}-test-data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '@...' mentions\n",
    "es.schema['data'].loc[:, 'text'] = es.schema['data'].loc[:, 'text']\\\n",
    "    .str.replace(r'(@[\\w]+ )', '', regex=True)\\\n",
    "    .apply(unidecode)\n",
    "\n",
    "text_analyzed = analyze(es.schema['data'].loc[:, ['id', 'text']], 'es')\n",
    "\n",
    "display(text_analyzed.head())\n",
    "\n",
    "# Entries without desired verb\n",
    "no_verb = ~(text_analyzed.loc[:, 'lemma'].str.contains(verb))\n",
    "print(f'Found {no_verb.sum()} without \"{verb}\"')\n",
    "\n",
    "display(pd.concat([es.schema['data'].loc[:, 'text'], es.schema['data'].loc[no_verb, :]], axis=1))\n",
    "\n",
    "es.join(to='data', data=text_analyzed, on='id')\n",
    "\n",
    "es.schema['data'].drop(es.schema['data'].loc[no_verb, :].index, inplace=True)\n",
    "response.reset_index()       \n",
    "    \n",
    "response.append(es)\n",
    "\n",
    "response.to_csv('es', verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "es_t = es_text.apply(unidecode)\n",
    "pt_t = pt_text.apply(unidecode)\n",
    "\n",
    "es_bad = is_bad_verb('es', es_t, 'vi OR ve OR ve')\n",
    "pt_bad = is_bad_verb('pt', pt_t, 'vi OR ve OR ve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_out = pd.concat(\n",
    "    [es.schema['data'].loc[:, 'text'], es_bad], axis=1)\n",
    "pt_out = pd.concat(\n",
    "    [pt.schema['data'].loc[:, 'text'], pt_bad], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('es', 'ver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac145c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_bad.loc[:, 'is_duplicate'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(es_out.head())\n",
    "display(pt_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bdbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = es_out.loc[~es_bad.loc[:, 'has_verb'], ['text', 'lemmad']]\n",
    "\n",
    "for i in range(b.shape[0]):\n",
    "    print(f'CASE:\\n')\n",
    "    print(f'ORIGINAL:\\n')\n",
    "    print(b.iloc[i, 0])\n",
    "    print(f'LEMMAD:\\n')\n",
    "    print(b.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f53451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
