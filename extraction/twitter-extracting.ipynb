{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31d8097",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<ul>\n",
    "    <li>Trending twitter<br></li>\n",
    "    https://twitter-trends.iamrohit.in/\n",
    "    <li>Removing accented characters<br></li>\n",
    "    https://stackoverflow.com/a/2633310/13557629\n",
    "    <li>Importing package from filepath</li>\n",
    "    https://stackoverflow.com/a/50395128/13557629\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "refined-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b01c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from unidecode import unidecode\n",
    "import es_dep_news_trf\n",
    "import pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c55a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util as imp\n",
    "import sys\n",
    "\n",
    "spec = imp.spec_from_file_location(\n",
    "    'twitter_connection', \n",
    "    '../twitter-connection/__init__.py')\n",
    "twit = imp.module_from_spec(spec)\n",
    "sys.modules[spec.name] = twit\n",
    "spec.loader.exec_module(twit)\n",
    "\n",
    "from twitter_connection import connection as tc\n",
    "from twitter_connection import response as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926a8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File path to the bearer token. Requires a prefix to identify the\n",
    "  token, which is just 'PERSONAL $BEARER_TOKEN$' by default -- \n",
    "  can be specified upon initialization of TwitterConnection\n",
    "'''\n",
    "cred_path = r'../twitter-connection/credentials.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f55f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the Portuguese-only verbs\n",
    "pt_verbs = {'dizer', 'supor', 'duvidar', 'acreditar', 'achar', 'lembrar', 'recear', 'predizer', 'adivinhar', 'conjeturar', 'chutar', 'dar(se) conta', 'desejar', 'oxalá', 'tomara'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a69fd3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../extraction/verb-stem-clean.txt') as f:\n",
    "    verb_stem = json.load(f)\n",
    "    \n",
    "verbs_volit = {vs[0]: vs[1] for vs in list(verb_stem.items())[len(verb_stem)-17:]}\n",
    "es_verbs_volit = {v: verbs_volit[v] for v in (verbs_volit.keys() - pt_verbs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd545c",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee21ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query_cond = 'lang:es has:geo -is:retweet -has:links '\n",
    "pt_query_cond = 'lang:pt has:geo -is:retweet -has:links '\n",
    "fields_tweet = 'tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets'\n",
    "fields_expan = 'expansions=author_id,geo.place_id,entities.mentions.username'\n",
    "fields_user = 'user.fields=created_at,location,public_metrics'\n",
    "fields_place = 'place.fields=country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9794d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF')\n",
    "\n",
    "pt_conn = tc.TwitterConnection(\n",
    "    is_archive=True,\n",
    "    cred_prefix='PROF') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30ab4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn.set_query(conditions=es_query_cond)\n",
    "es_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)\n",
    "\n",
    "pt_conn.set_query(conditions=pt_query_cond)\n",
    "pt_conn.set_fields(tweet=fields_tweet, \n",
    "                      expansions=fields_expan, \n",
    "                      user=fields_user,\n",
    "                      place=fields_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b6c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_es = es_dep_news_trf.load()\n",
    "nlp_pt = pt_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe9e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(tokenized):\n",
    "    return ' '.join([f'{t.text}-({t.pos_})' for t in tokenized if ((t.pos_!='PUNCT') and (t.pos_!='SPACE'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01b01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokenized):\n",
    "    return ' '.join([t.lemma_ for t in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2353878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text, lang):\n",
    "    # Tokenized\n",
    "    text_nlp = text.loc[:, 'text'].apply(nlp_es if lang=='es' else nlp_pt)\n",
    "    \n",
    "    pos = text_nlp.apply(get_pos_tags).rename('pos')\n",
    "    lemma = text_nlp.apply(lemmatize).rename('lemma')\n",
    "    \n",
    "    return pd.concat([text.loc[:, 'id'], pos, lemma], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b75702",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_verbs = es_verb_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20977bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0, 'jur', 'dig OR dec OR dij OR dir OR dic', 'confies OR confes', 'promet', 'muestr OR mostr', 'demuestr OR demostr', 'afirm', 'confirm', 'consig OR conseg', 'logr', 'respond', 'admit', 'consider', 'aesgur', 'mencion', 'grit', 'suspir', 'reclam', 'cont OR cuent', 'supon', 'sé OR sab OR sup OR sep', 'piens OR pens', 'imagin', 'dud', 'cre OR kre', 'recuerd OR record', 'acuerdo OR acord', 'tem', 'recomiend OR recomend', 'parec OR parezc OR pareç', 'entiend OR entend', 'neg OR nieg', 'apuest OR apost', 'predig OR predic OR predec OR predij', 'preve', 'sient OR sint OR sent', 'comprueb OR comprob OR comprov', 'adivin', 0, 'lament', 'rueg OR rog', 'quer OR quier OR quis', 'esper', 'dese', 'pid OR ped OR peç', 'ojalá OR ohalá', 'suplic OR supliq', 'solicit', 'mand', 'orden', 'insist', 'sugier OR suger OR sugir', 'preocup', 'alegr', 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delinqs = ['ver', 'dar(se) cuenta']\n",
    "\n",
    "for v in delinqs:\n",
    "    es_verbs[v] = 0\n",
    "\n",
    "es_verbs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "382d95a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving tweets: jurar\n",
      "Topics: (jur)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: decir\n",
      "Topics: (dig OR dec OR dij OR dir OR dic)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(dig OR dec OR dij OR dir OR dic) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Found 93 without \"decir\"\n",
      "Found 95 without \"decir\"\n",
      "Before append: 7\n",
      "After append: 12\n",
      "Found 98 without \"decir\"\n",
      "Before append: 12\n",
      "After append: 14\n",
      "Found 94 without \"decir\"\n",
      "Before append: 14\n",
      "After append: 19\n",
      "Found 93 without \"decir\"\n",
      "Before append: 19\n",
      "After append: 26\n",
      "Found 95 without \"decir\"\n",
      "Before append: 26\n",
      "After append: 31\n",
      "No next token! next_token\n",
      "Retrieving tweets: confesar\n",
      "Topics: (confies OR confes)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(confies OR confes) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Found 99 without \"confesar\"\n",
      "No next token! next_token\n",
      "Retrieving tweets: prometer\n",
      "Topics: (promet)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: mostrar\n",
      "Topics: (muestr OR mostr)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: demostrar\n",
      "Topics: (demuestr OR demostr)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: afirmar\n",
      "Topics: (afirm)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: confirmar\n",
      "Topics: (confirm)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: conseguir\n",
      "Topics: (consig OR conseg)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: lograr\n",
      "Topics: (logr)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: responder\n",
      "Topics: (respond)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: admitir\n",
      "Topics: (admit)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: considerar\n",
      "Topics: (consider)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: asegurar\n",
      "Topics: (aesgur)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: mencionar\n",
      "Topics: (mencion)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(mencion) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Found 99 without \"mencionar\"\n",
      "Found 99 without \"mencionar\"\n",
      "Before append: 0\n",
      "After append: 0\n",
      "Found 100 without \"mencionar\"\n",
      "Before append: 0\n",
      "After append: 0\n",
      "Found 98 without \"mencionar\"\n",
      "Before append: 0\n",
      "After append: 2\n",
      "Found 99 without \"mencionar\"\n",
      "Before append: 2\n",
      "After append: 3\n",
      "Found 100 without \"mencionar\"\n",
      "Before append: 3\n",
      "After append: 3\n",
      "Found 100 without \"mencionar\"\n",
      "Before append: 3\n",
      "After append: 3\n",
      "Found 98 without \"mencionar\"\n",
      "Before append: 3\n",
      "After append: 3\n",
      "Found 97 without \"mencionar\"\n",
      "Before append: 3\n",
      "After append: 5\n",
      "No next token! next_token\n",
      "Retrieving tweets: gritar\n",
      "Topics: (grit)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: suspirar\n",
      "Topics: (suspir)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: reclamar\n",
      "Topics: (reclam)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: contar\n",
      "Topics: (cont OR cuent)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(cont OR cuent) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Found 96 without \"contar\"\n",
      "No next token! next_token\n",
      "Retrieving tweets: suponer\n",
      "Topics: (supon)\n",
      "No next token! next_token\n",
      "Failed to rename some columns -- not found?\n",
      "Exception during merge for CSV! data\n",
      "Retrieving tweets: saber\n",
      "Topics: (sé OR sab OR sup OR sep)\n",
      "https://api.twitter.com/2/tweets/search/all?query=(sé OR sab OR sup OR sep) lang:es has:geo -is:retweet -has:links &max_results=100&tweet.fields=lang,geo,created_at,public_metrics,referenced_tweets&expansions=author_id,geo.place_id,entities.mentions.username&user.fields=created_at,location,public_metrics&place.fields=country\n",
      "Found 94 without \"saber\"\n",
      "Found 93 without \"saber\"\n",
      "Before append: 6\n",
      "After append: 13\n",
      "Found 97 without \"saber\"\n",
      "Before append: 13\n",
      "After append: 16\n",
      "Found 96 without \"saber\"\n",
      "Before append: 16\n",
      "After append: 20\n",
      "Found 100 without \"saber\"\n",
      "Before append: 20\n",
      "After append: 20\n",
      "Found 94 without \"saber\"\n",
      "Before append: 20\n",
      "After append: 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-88e832e56f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtext_analyzed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'es'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Entries without desired verb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-90bdb551b4c0>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_es\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'es'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnlp_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pos_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    993\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m         \u001b[0minstall_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTransformerBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \"\"\"\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/spacy_transformers/layers/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"logger\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mlog_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"after forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    287\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/thinc/layers/pytorchwrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mYtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_backprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dYtorch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArgsKwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         )\n\u001b[1;32m    499\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for vs in es_verbs.items():\n",
    "    if vs[1]==0:\n",
    "        continue\n",
    "    \n",
    "    verb = vs[0]\n",
    "    stems = '(' + vs[1] + ')'\n",
    "    print(f'Retrieving tweets: {verb}\\nTopics: {stems}')\n",
    "    \n",
    "    response = tr.Response()\n",
    "    \n",
    "    while es_conn.connect(stems, is_next=True, time_interval=1):\n",
    "        if len(response.schema)==0:\n",
    "            print(es_conn.url)\n",
    "        \n",
    "        new = tr.Response(es_conn.response)\n",
    "        \n",
    "        # Remove '@...' mentions\n",
    "        new.schema['data'].loc[:, 'text'] = new.schema['data'].loc[:, 'text']\\\n",
    "            .str.replace(r'(@[\\w]+ )', '', regex=True)\\\n",
    "            .apply(unidecode)\n",
    "        \n",
    "        text_analyzed = analyze(new.schema['data'].loc[:, ['id', 'text']], 'es')\n",
    "        \n",
    "        # Entries without desired verb\n",
    "        no_verb = ~(text_analyzed.loc[:, 'lemma'].str.contains(verb))\n",
    "        print(f'Found {no_verb.sum()} without \"{verb}\"')\n",
    "        \n",
    "        new.join(to='data', data=text_analyzed, on='id')\n",
    "        \n",
    "        new.schema['data'].drop(new.schema['data'].loc[no_verb, :].index, inplace=True)\n",
    "        response.reset_index()       \n",
    "            \n",
    "        response.append(new)\n",
    "        \n",
    "        if response.schema['data'].shape[0] >= 200:\n",
    "            break\n",
    "    \n",
    "    response.to_csv('es', verb)\n",
    "    es_verbs[verb] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47499b",
   "metadata": {},
   "source": [
    "### Extracting Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d32379d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 22 convos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_ids = response.loc[:, 'conversation_id'].values\n",
    "\n",
    "print(f'Extracted {len(conv_ids)} convos\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b15801",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_conv = 'conversation_id:'\n",
    "query_conv_cond = ' -has:links '\n",
    "conv_fields = 'tweet.fields=conversation_id,lang,in_reply_to_user_id,text,attachments,public_metrics'\\\n",
    "  '&expansions=author_id&user.fields=username,public_metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7311abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for conversations\n",
    "convs = pd.DataFrame()\n",
    "# Dataframes for users\n",
    "users = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0cc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the rate limit is hit while querying, save progress\n",
    "convo_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbef14b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402642305522282500\n",
      "No next token!\n",
      "1402637774805667841\n",
      "No next token!\n",
      "1402634503173165058\n",
      "No next token!\n",
      "1402624093946527745\n",
      "1402634186909962240\n",
      "No next token!\n",
      "1402634112557518854\n",
      "No next token!\n",
      "1402633756888944642\n",
      "No next token!\n",
      "1402631188284645382\n",
      "1402633207640641538\n",
      "No next token!\n",
      "1402626496989179915\n",
      "1402629532994125827\n",
      "No next token!\n",
      "1402615477172686850\n",
      "No next token!\n",
      "1402625048180047879\n",
      "No next token!\n",
      "1402624924129366018\n",
      "No next token!\n",
      "1402617093514727426\n",
      "1402131777641193477\n",
      "No next token!\n",
      "1402620991369535495\n",
      "No next token!\n",
      "1402619563200729092\n",
      "No next token!\n",
      "1402623588176478209\n",
      "No next token!\n",
      "1402623338535604229\n",
      "No next token!\n",
      "1402623254767050756\n",
      "No next token!\n",
      "1402454421439717378\n",
      "No next token!\n"
     ]
    }
   ],
   "source": [
    "for c_id in conv_ids[convo_idx:]:\n",
    "    d = pd.DataFrame()\n",
    "    u = pd.DataFrame()\n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    while connection.connect(query_conv + c_id + query_conv_cond, conv_fields, True):\n",
    "        d = d.append(\n",
    "            pd.json_normalize(connection.response, record_path='data'))\n",
    "        u = u.append(\n",
    "            pd.json_normalize(connection.response['includes'], record_path='users'))\n",
    "        \n",
    "        d.loc[:, 'text'] = d.loc[:, 'text'].str.replace(r'^(@[\\w]+ )+', '', regex=True)\n",
    "        \n",
    "        short_text = d.loc[:, 'text'].apply(len) < 20\n",
    "        lang = d.loc[:, 'lang']!='es'\n",
    "        \n",
    "        d.drop(\n",
    "            d.loc[lang|short_text].index, \n",
    "            inplace=True)\n",
    "        \n",
    "        if d.shape[0]>14:\n",
    "            break\n",
    "        \n",
    "    convs = convs.append(d, ignore_index=True)\n",
    "    users = users.append(u, ignore_index=True)\n",
    "    \n",
    "    convo_idx+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b023b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 109 texts\n",
      "Sample:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69416519</td>\n",
       "      <td>1402624093946527745</td>\n",
       "      <td>1537353780</td>\n",
       "      <td>es</td>\n",
       "      <td>Muchas fuerza Florencia,lo que te desea el mal...</td>\n",
       "      <td>1402658383950454791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69416519</td>\n",
       "      <td>1402624093946527745</td>\n",
       "      <td>468895557</td>\n",
       "      <td>es</td>\n",
       "      <td>El mal que hace la madre lo pagan los hijos.\\n...</td>\n",
       "      <td>1402656218150285315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69416519</td>\n",
       "      <td>1402624093946527745</td>\n",
       "      <td>1471308854</td>\n",
       "      <td>es</td>\n",
       "      <td>Nunca un hospital público estos garcas. Ni ver...</td>\n",
       "      <td>1402655563792764935</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69416519</td>\n",
       "      <td>1402624093946527745</td>\n",
       "      <td>398461357</td>\n",
       "      <td>es</td>\n",
       "      <td>Porque no muestra la cara ?</td>\n",
       "      <td>1402653558219194372</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69416519</td>\n",
       "      <td>1402624093946527745</td>\n",
       "      <td>169167606</td>\n",
       "      <td>es</td>\n",
       "      <td>Esta mujer esta anoréxica</td>\n",
       "      <td>1402653194174468103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  in_reply_to_user_id      conversation_id   author_id lang  \\\n",
       "0            69416519  1402624093946527745  1537353780   es   \n",
       "1            69416519  1402624093946527745   468895557   es   \n",
       "2            69416519  1402624093946527745  1471308854   es   \n",
       "3            69416519  1402624093946527745   398461357   es   \n",
       "4            69416519  1402624093946527745   169167606   es   \n",
       "\n",
       "                                                text                   id  \\\n",
       "0  Muchas fuerza Florencia,lo que te desea el mal...  1402658383950454791   \n",
       "1  El mal que hace la madre lo pagan los hijos.\\n...  1402656218150285315   \n",
       "2  Nunca un hospital público estos garcas. Ni ver...  1402655563792764935   \n",
       "3                        Porque no muestra la cara ?  1402653558219194372   \n",
       "4                          Esta mujer esta anoréxica  1402653194174468103   \n",
       "\n",
       "   public_metrics.retweet_count  public_metrics.reply_count  \\\n",
       "0                             0                           0   \n",
       "1                             0                           0   \n",
       "2                             0                           0   \n",
       "3                             0                           0   \n",
       "4                             0                           0   \n",
       "\n",
       "   public_metrics.like_count  public_metrics.quote_count  \n",
       "0                          0                           0  \n",
       "1                          0                           0  \n",
       "2                          0                           0  \n",
       "3                          0                           0  \n",
       "4                          1                           0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Extracted {convs.shape[0]} texts\\nSample:\\n')\n",
    "convs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e8c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('es_convs.txt', 'w') as d:\n",
    "    d.writelines(convs.to_json(orient='table', force_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290ed0e",
   "metadata": {},
   "source": [
    "#### Examining extracted users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad2435",
   "metadata": {},
   "source": [
    "## TODO: users were improperly counted; count is WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7c3196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 256, duplicated: 27\n"
     ]
    }
   ],
   "source": [
    "dups = users.loc[:, 'id'].duplicated()\n",
    "\n",
    "print(f'Users: {users.shape[0]}, duplicated: {dups.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ada6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "users.drop(users.loc[dups].index, inplace=True)\n",
    "print(users.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31b1bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('es_users.txt', 'w') as d:\n",
    "    d.writelines(users.to_json(orient='table', force_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255e71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
