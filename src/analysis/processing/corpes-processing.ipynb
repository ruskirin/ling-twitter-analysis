{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7476d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "import yaml\n",
    "import logging\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import processing\n",
    "from unidecode import unidecode\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "# spacy didn't work without this import (bug)\n",
    "from torch.utils import dlpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ac5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "spec_src = importlib.util.spec_from_file_location(\n",
    "    'src', \n",
    "    '../../__init__.py')\n",
    "m = importlib.util.module_from_spec(spec_src)\n",
    "sys.modules[spec_src.name] = m\n",
    "spec_src.loader.exec_module(m)\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ebe85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.get_logger('processing-corpes-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb906a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'processing' from '/home/rimov/Documents/Code/NLP/lin-que-dropping/src/analysis/processing/processing.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reload module\n",
    "\"\"\"\n",
    "importlib.reload(processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e5487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_conf = utils.get_config()\n",
    "conf = utils.get_config('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33de11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conj_path = utils.get_project_root()/gen_conf['file_paths']['verb_conjug']\n",
    "cleaned_folder = '12062021'\n",
    "cleaned_path = utils.get_save_path('c', 'corpes', lang='es', is_test=True)/cleaned_folder\n",
    "processed_folder = cleaned_folder\n",
    "processed_path = utils.get_save_path('p', 'corpes', lang='es', is_test=True)/processed_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4aaa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_type</th>\n",
       "      <th>verb</th>\n",
       "      <th>indicativo</th>\n",
       "      <th>imperativo</th>\n",
       "      <th>subjuntivo</th>\n",
       "      <th>gerundio</th>\n",
       "      <th>gerundio_compuesto</th>\n",
       "      <th>infinitivo</th>\n",
       "      <th>infinitivo_compuesto</th>\n",
       "      <th>participio_pasado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stative</td>\n",
       "      <td>ver</td>\n",
       "      <td>veía  visto  verías  vi  vimos  verían  ves  v...</td>\n",
       "      <td>vean   ve   vea   veamos   ved</td>\n",
       "      <td>veáis  visto  vieras  vieren  viesen  veas  vi...</td>\n",
       "      <td>viendo</td>\n",
       "      <td>visto</td>\n",
       "      <td>ver</td>\n",
       "      <td>visto</td>\n",
       "      <td>visto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stative</td>\n",
       "      <td>jurar</td>\n",
       "      <td>jurarán  juramos  jurarías  jurabas  juraría  ...</td>\n",
       "      <td>jurad   jura   juren   jure   juremos</td>\n",
       "      <td>jurare  jurareis  jurase  jurara  juraren  jur...</td>\n",
       "      <td>jurando</td>\n",
       "      <td>jurado</td>\n",
       "      <td>jurar</td>\n",
       "      <td>jurado</td>\n",
       "      <td>jurado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  verb_type   verb                                         indicativo  \\\n",
       "0   Stative    ver  veía  visto  verías  vi  vimos  verían  ves  v...   \n",
       "1   Stative  jurar  jurarán  juramos  jurarías  jurabas  juraría  ...   \n",
       "\n",
       "                               imperativo  \\\n",
       "0         vean   ve   vea   veamos   ved    \n",
       "1  jurad   jura   juren   jure   juremos    \n",
       "\n",
       "                                          subjuntivo gerundio  \\\n",
       "0  veáis  visto  vieras  vieren  viesen  veas  vi...   viendo   \n",
       "1  jurare  jurareis  jurase  jurara  juraren  jur...  jurando   \n",
       "\n",
       "  gerundio_compuesto infinitivo infinitivo_compuesto participio_pasado  \n",
       "0              visto        ver                visto             visto  \n",
       "1             jurado      jurar               jurado            jurado  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "es_conjugs = pd.read_excel(es_conj_path)\n",
    "display(es_conjugs.head(2))\n",
    "\n",
    "es_verbs = set(es_conjugs['verb'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414717",
   "metadata": {},
   "source": [
    "### Running through spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb6a0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the Named Entity Recognizer\n",
    "nlp_es = spacy.load(conf['spacy']['es'], disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0ceb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normd(tokenized):\n",
    "    normd = ''\n",
    "    \n",
    "    for t in tokenized:\n",
    "        token = unidecode(t.text).lower()\n",
    "        \n",
    "        if t.pos_=='PUNCT':\n",
    "            normd+=f'{t.text}'\n",
    "            continue\n",
    "        \n",
    "        if token=='que' or token=='q':\n",
    "            normd+=f' {t.text.upper()}'\n",
    "            continue\n",
    "        \n",
    "        if (t.pos_=='VERB') and (t.lemma_ in es_verbs) and (t.dep_=='ccomp'):\n",
    "            normd+=f' <<{t.text.upper()}>>'\n",
    "            continue\n",
    "        \n",
    "        normd+=f' {t.text.lower()}'\n",
    "    \n",
    "    return normd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2b235a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_ccomp(tokenized):\n",
    "    has = any([t.dep_=='ccomp' for t in tokenized])\n",
    "    return 'TRUE' if has else 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4491c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(tokenized):\n",
    "    verbs = ', '.join(set(t.lemma_ for t in tokenized if (t.pos_=='VERB') and (t.lemma_ in es_verbs)))\n",
    "    return verbs if len(verbs)>0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82441d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep(tokenized):\n",
    "    deps = ''\n",
    "    \n",
    "    for t in tokenized:\n",
    "        if t.pos_=='PUNCT':\n",
    "            deps+=f' {t.text}'\n",
    "            continue\n",
    "        \n",
    "        deps+=f' {t.text.lower()}[{t.dep_}]'\n",
    "    \n",
    "    return deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fea6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(tokenized):\n",
    "    return ' '.join([f'{t.text}({t.pos_.upper()})' for t in tokenized if t.pos_!='PUNCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78d1b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(tokenized):\n",
    "    return ' '.join([f'<{t.text}>({t.lemma_.lower()},{t.is_stop})' for t in tokenized if t.pos_!='PUNCT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03f44917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_verbs(df):\n",
    "    have = df['CONCORDANCIA'].apply(get_verbs).notna()\n",
    "    return df.loc[have, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a423f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(tokenized: list, file_path, file_name):\n",
    "    batch = have_verbs(pd.concat(tokenized, ignore_index=False))\n",
    "    \n",
    "    verbs = batch['CONCORDANCIA'].apply(get_verbs).rename('verbs')\n",
    "    normd = batch['CONCORDANCIA'].apply(get_normd).rename('normalized')\n",
    "    ccomp = batch['CONCORDANCIA'].apply(has_ccomp).rename('has_ccomp')\n",
    "    dep = batch['CONCORDANCIA'].apply(get_dep).rename('dependencies')\n",
    "    pos = batch['CONCORDANCIA'].apply(get_pos).rename('pos')\n",
    "    details = batch['CONCORDANCIA'].apply(get_details).rename('details')\n",
    "    \n",
    "    batch = pd.concat([verbs, batch['CONCORDANCIA'], normd, ccomp, dep, pos, details], axis=1)\n",
    "    \n",
    "    utils.save_csv(file_path, batch, file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b518ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rimov/Documents/Code/NLP/lin-que-dropping/processing/../processing/saved/corpes/es/12062021')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.make_dir(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "668d13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58c177c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rogar', 'solicitar', 'suplicar'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bc185f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-2051-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 5: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-2501-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 301: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-2601-corpes-2-7-21-sk-processed\n",
      "esperar-1-corpes-2-7-21-sk-processed\n",
      "esperar-1301-corpes-2-7-21-sk-processed\n",
      "esperar-1451-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 148: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-2151-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 869: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-3651-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 889: expected 14 fields, saw 27\\nSkipping line 962: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-4751-corpes-2-7-21-sk-processed\n",
      "esperar-4701-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 30: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-3951-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 223: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-4451-corpes-2-7-21-sk-processed\n",
      "esperar-3801-corpes-2-7-21-sk-processed\n",
      "esperar-3551-corpes-2-7-21-sk-processed\n",
      "esperar-4351-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 624: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-1751-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 833: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-4651-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 591: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-3601-corpes-2-7-21-sk-processed\n",
      "esperar-951-corpes-2-7-21-sk-processed\n",
      "esperar-1501-corpes-2-7-21-sk-processed\n",
      "esperar-851-corpes-2-7-21-sk-processed\n",
      "esperar-101-corpes-2-7-21-sk-processed\n",
      "esperar-1351-corpes-2-7-21-sk-processed\n",
      "esperar-3101-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 74: expected 14 fields, saw 27\\nSkipping line 668: expected 14 fields, saw 40\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-1201-corpes-2-7-21-sk-processed\n",
      "esperar-2551-corpes-2-7-21-sk-processed\n",
      "esperar-4951-corpes-2-7-21-sk-processed\n",
      "esperar-501-corpes-2-7-21-sk-processed\n",
      "esperar-1401-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 571: expected 14 fields, saw 27\\nSkipping line 985: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-3351-corpes-2-7-21-sk-processed\n",
      "esperar-651-corpes-2-7-21-sk-processed\n",
      "esperar-351-corpes-2-7-21-sk-processed\n",
      "esperar-1801-corpes-2-7-21-sk-processed\n",
      "esperar-3201-corpes-2-7-21-sk-processed\n",
      "esperar-2451-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 717: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-1151-corpes-2-7-21-sk-processed\n",
      "esperar-1901-corpes-2-7-21-sk-processed\n",
      "esperar-1701-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 299: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-4551-corpes-2-7-21-sk-processed\n",
      "esperar-3251-corpes-2-7-21-sk-processed\n",
      "esperar-2801-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 289: expected 14 fields, saw 27\\nSkipping line 398: expected 14 fields, saw 27\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esperar-401-corpes-2-7-21-sk-processed\n",
      "esperar-901-corpes-2-7-21-sk-processed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Integer column has NA values in column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Code/NLP/lin-que-dropping/processing/../twitter-connection/util/utils.py\u001b[0m in \u001b[0;36mget_csv\u001b[0;34m(data_from, path, dtypes, dates, sep, lineterminator)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         data = pd.read_csv(path,\n\u001b[0m\u001b[1;32m    101\u001b[0m                            \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'on_bad_lines'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12593/1154537742.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'corpes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         procd = [pd.concat([data.iloc[:, :-1], \n",
      "\u001b[0;32m~/Documents/Code/NLP/lin-que-dropping/processing/../twitter-connection/util/utils.py\u001b[0m in \u001b[0;36mget_csv\u001b[0;34m(data_from, path, dtypes, dates, sep, lineterminator)\u001b[0m\n\u001b[1;32m    105\u001b[0m                            on_bad_lines='warn')\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         data = pd.read_csv(path,\n\u001b[0m\u001b[1;32m    108\u001b[0m                            \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                            \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Integer column has NA values in column 3"
     ]
    }
   ],
   "source": [
    "for verb in cleaned_path.iterdir():\n",
    "    if verb.stem in finished:\n",
    "        continue \n",
    "        \n",
    "    save_path = processed_path/verb.stem\n",
    "    utils.make_dir(save_path)\n",
    "    \n",
    "    for file in verb.iterdir():\n",
    "        file_name = file.stem.lower()+'-processed'\n",
    "        print(file_name)\n",
    "        \n",
    "        data = utils.get_csv(data_from='corpes', path=file, sep='\\t')\n",
    "        \n",
    "        procd = [pd.concat([data.iloc[:, :-1], \n",
    "                            data.loc[:, 'CONCORDANCIA'].apply(nlp_es)], \n",
    "                           axis=1)]\n",
    "        save_batch(procd, file_path=save_path, file_name=file_name)\n",
    "    \n",
    "    finished.add(verb.stem)\n",
    "    print(f'Processed: {verb.stem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44a24af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbs</th>\n",
       "      <th>CONCORDANCIA</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>lemma_pos_stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esperar, contar</td>\n",
       "      <td>Nancy Ramos es una niña que necesita un transp...</td>\n",
       "      <td>Nancy[nsubj] Ramos[flat] es[cop] una[det] niña...</td>\n",
       "      <td>&lt;&lt;Nancy&gt;&gt;(Nancy,PROPN,False) &lt;&lt;Ramos&gt;&gt;(Ramos,P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esperar, querer</td>\n",
       "      <td>No. Yo quiero, amo y espero lo mismo.</td>\n",
       "      <td>No[ROOT] .[] Yo[nsubj] quiero[ROOT] ,[] amo[ad...</td>\n",
       "      <td>&lt;&lt;No&gt;&gt;(no,ADV,True) &lt;&lt;Yo&gt;&gt;(yo,PRON,True) &lt;&lt;qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             verbs                                       CONCORDANCIA  \\\n",
       "0  esperar, contar  Nancy Ramos es una niña que necesita un transp...   \n",
       "1  esperar, querer              No. Yo quiero, amo y espero lo mismo.   \n",
       "\n",
       "                                        dependencies  \\\n",
       "0  Nancy[nsubj] Ramos[flat] es[cop] una[det] niña...   \n",
       "1  No[ROOT] .[] Yo[nsubj] quiero[ROOT] ,[] amo[ad...   \n",
       "\n",
       "                                  lemma_pos_stopword  \n",
       "0  <<Nancy>>(Nancy,PROPN,False) <<Ramos>>(Ramos,P...  \n",
       "1  <<No>>(no,ADV,True) <<Yo>>(yo,PRON,True) <<qui...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = utils.get_csv('corpes', processed_path/verb.stem/'esperar-1-corpes-2-7-21-sk-processed.csv')\n",
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d475631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbs</th>\n",
       "      <th>CONCORDANCIA</th>\n",
       "      <th>dependencies</th>\n",
       "      <th>lemma_pos_stopword</th>\n",
       "      <th>dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esperar, contar</td>\n",
       "      <td>Nancy Ramos es una niña que necesita un transp...</td>\n",
       "      <td>Nancy[nsubj] Ramos[flat] es[cop] una[det] niña...</td>\n",
       "      <td>&lt;&lt;Nancy&gt;&gt;(Nancy,PROPN,False) &lt;&lt;Ramos&gt;&gt;(Ramos,P...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esperar, querer</td>\n",
       "      <td>No. Yo quiero, amo y espero lo mismo.</td>\n",
       "      <td>No[ROOT] .[] Yo[nsubj] quiero[ROOT] ,[] amo[ad...</td>\n",
       "      <td>&lt;&lt;No&gt;&gt;(no,ADV,True) &lt;&lt;Yo&gt;&gt;(yo,PRON,True) &lt;&lt;qui...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esperar</td>\n",
       "      <td>Mañana mismo voy a Valencia, a la gala de Miss...</td>\n",
       "      <td>Mañana[advmod] mismo[advmod] voy[ROOT] a[case]...</td>\n",
       "      <td>&lt;&lt;Mañana&gt;&gt;(mañana,ADV,False) &lt;&lt;mismo&gt;&gt;(mismo,A...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>esperar</td>\n",
       "      <td>La preparo este año y el próximo año daré unos...</td>\n",
       "      <td>La[obj] preparo[ROOT] este[det] año[obl] y[cc]...</td>\n",
       "      <td>&lt;&lt;La&gt;&gt;(él,PRON,True) &lt;&lt;preparo&gt;&gt;(preparar,VERB...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esperar</td>\n",
       "      <td>16. ¿Qué espera de esta nueva novela?</td>\n",
       "      <td>16[ROOT] .[] ¿[] Qué[obj] espera[ROOT] de[case...</td>\n",
       "      <td>&lt;&lt;16&gt;&gt;(16,NUM,False) &lt;&lt;Qué&gt;&gt;(qué,PRON,True) &lt;&lt;...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             verbs                                       CONCORDANCIA  \\\n",
       "0  esperar, contar  Nancy Ramos es una niña que necesita un transp...   \n",
       "1  esperar, querer              No. Yo quiero, amo y espero lo mismo.   \n",
       "2          esperar  Mañana mismo voy a Valencia, a la gala de Miss...   \n",
       "3          esperar  La preparo este año y el próximo año daré unos...   \n",
       "4          esperar              16. ¿Qué espera de esta nueva novela?   \n",
       "\n",
       "                                        dependencies  \\\n",
       "0  Nancy[nsubj] Ramos[flat] es[cop] una[det] niña...   \n",
       "1  No[ROOT] .[] Yo[nsubj] quiero[ROOT] ,[] amo[ad...   \n",
       "2  Mañana[advmod] mismo[advmod] voy[ROOT] a[case]...   \n",
       "3  La[obj] preparo[ROOT] este[det] año[obl] y[cc]...   \n",
       "4  16[ROOT] .[] ¿[] Qué[obj] espera[ROOT] de[case...   \n",
       "\n",
       "                                  lemma_pos_stopword  dependencies  \n",
       "0  <<Nancy>>(Nancy,PROPN,False) <<Ramos>>(Ramos,P...         False  \n",
       "1  <<No>>(no,ADV,True) <<Yo>>(yo,PRON,True) <<qui...         False  \n",
       "2  <<Mañana>>(mañana,ADV,False) <<mismo>>(mismo,A...         False  \n",
       "3  <<La>>(él,PRON,True) <<preparo>>(preparar,VERB...         False  \n",
       "4  <<16>>(16,NUM,False) <<Qué>>(qué,PRON,True) <<...         False  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_ccomp = sample['dependencies'].str.contains('ccomp')\n",
    "sample = pd.concat([sample, has_ccomp], axis=1)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5225d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_excel(test_path, sample, 'esperar-1-corpes-2-7-21-sk-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "933b3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dab187",
   "metadata": {},
   "source": [
    "### TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea2bb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = utils.get_save_path('c', 'corpes', lang='es', is_test=False)/'12062021'/'esperar'\n",
    "test_file = 'ESPERAR-1-CORPES-2-7-21-SK.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f44000c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>BIBLIOGRAFÍA</th>\n",
       "      <th>AUTOR</th>\n",
       "      <th>TÍTULO</th>\n",
       "      <th>FECHA</th>\n",
       "      <th>CRITERIO</th>\n",
       "      <th>BLOQUE</th>\n",
       "      <th>MEDIO</th>\n",
       "      <th>SOPORTE</th>\n",
       "      <th>TEMA</th>\n",
       "      <th>PAÍS</th>\n",
       "      <th>ZONA</th>\n",
       "      <th>TIPOLOGÍA</th>\n",
       "      <th>NOTAS</th>\n",
       "      <th>CONCORDANCIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne. ww...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne</td>\n",
       "      <td>2001</td>\n",
       "      <td>Fecha de escritura</td>\n",
       "      <td>No ficción</td>\n",
       "      <td>Escrito</td>\n",
       "      <td>Web</td>\n",
       "      <td>Artes, cultura y espectáculos</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Antillas</td>\n",
       "      <td>Entrevista digital</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Nancy Ramos es una niña que necesita un transp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne. ww...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne</td>\n",
       "      <td>2001</td>\n",
       "      <td>Fecha de escritura</td>\n",
       "      <td>No ficción</td>\n",
       "      <td>Escrito</td>\n",
       "      <td>Web</td>\n",
       "      <td>Artes, cultura y espectáculos</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Antillas</td>\n",
       "      <td>Entrevista digital</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No. Yo quiero, amo y espero lo mismo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne. ww...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne</td>\n",
       "      <td>2001</td>\n",
       "      <td>Fecha de escritura</td>\n",
       "      <td>No ficción</td>\n",
       "      <td>Escrito</td>\n",
       "      <td>Web</td>\n",
       "      <td>Artes, cultura y espectáculos</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Antillas</td>\n",
       "      <td>Entrevista digital</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Mañana mismo voy a Valencia, a la gala de Miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne. ww...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Chayanne</td>\n",
       "      <td>2001</td>\n",
       "      <td>Fecha de escritura</td>\n",
       "      <td>No ficción</td>\n",
       "      <td>Escrito</td>\n",
       "      <td>Web</td>\n",
       "      <td>Artes, cultura y espectáculos</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Antillas</td>\n",
       "      <td>Entrevista digital</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>La preparo este año y el próximo año daré unos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Laura Esquiv...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Elmundo.es. Encuentro digital con Laura Esquivel</td>\n",
       "      <td>2001</td>\n",
       "      <td>Fecha de escritura</td>\n",
       "      <td>No ficción</td>\n",
       "      <td>Escrito</td>\n",
       "      <td>Web</td>\n",
       "      <td>Artes, cultura y espectáculos</td>\n",
       "      <td>México</td>\n",
       "      <td>México y Centroamérica</td>\n",
       "      <td>Entrevista digital</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>16. ¿Qué espera de esta nueva novela?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       BIBLIOGRAFÍA AUTOR  \\\n",
       "0   0  Elmundo.es. Encuentro digital con Chayanne. ww...  <NA>   \n",
       "1   1  Elmundo.es. Encuentro digital con Chayanne. ww...  <NA>   \n",
       "2   2  Elmundo.es. Encuentro digital con Chayanne. ww...  <NA>   \n",
       "3   3  Elmundo.es. Encuentro digital con Chayanne. ww...  <NA>   \n",
       "4   4  Elmundo.es. Encuentro digital con Laura Esquiv...  <NA>   \n",
       "\n",
       "                                             TÍTULO FECHA            CRITERIO  \\\n",
       "0        Elmundo.es. Encuentro digital con Chayanne  2001  Fecha de escritura   \n",
       "1        Elmundo.es. Encuentro digital con Chayanne  2001  Fecha de escritura   \n",
       "2        Elmundo.es. Encuentro digital con Chayanne  2001  Fecha de escritura   \n",
       "3        Elmundo.es. Encuentro digital con Chayanne  2001  Fecha de escritura   \n",
       "4  Elmundo.es. Encuentro digital con Laura Esquivel  2001  Fecha de escritura   \n",
       "\n",
       "       BLOQUE    MEDIO SOPORTE                           TEMA         PAÍS  \\\n",
       "0  No ficción  Escrito     Web  Artes, cultura y espectáculos  Puerto Rico   \n",
       "1  No ficción  Escrito     Web  Artes, cultura y espectáculos  Puerto Rico   \n",
       "2  No ficción  Escrito     Web  Artes, cultura y espectáculos  Puerto Rico   \n",
       "3  No ficción  Escrito     Web  Artes, cultura y espectáculos  Puerto Rico   \n",
       "4  No ficción  Escrito     Web  Artes, cultura y espectáculos       México   \n",
       "\n",
       "                     ZONA           TIPOLOGÍA NOTAS  \\\n",
       "0                Antillas  Entrevista digital  <NA>   \n",
       "1                Antillas  Entrevista digital  <NA>   \n",
       "2                Antillas  Entrevista digital  <NA>   \n",
       "3                Antillas  Entrevista digital  <NA>   \n",
       "4  México y Centroamérica  Entrevista digital  <NA>   \n",
       "\n",
       "                                        CONCORDANCIA  \n",
       "0  Nancy Ramos es una niña que necesita un transp...  \n",
       "1              No. Yo quiero, amo y espero lo mismo.  \n",
       "2  Mañana mismo voy a Valencia, a la gala de Miss...  \n",
       "3  La preparo este año y el próximo año daré unos...  \n",
       "4              16. ¿Qué espera de esta nueva novela?  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = utils.get_csv('corpes', test_path/test_file, sep='\\t')\n",
    "data = data.reset_index()\n",
    "data = data.rename({'index': 'id'}, axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92b23ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(tokenized: list, file_path, file_name):\n",
    "    batch = have_verbs(pd.concat(tokenized, ignore_index=False))\n",
    "    \n",
    "    verbs = batch['CONCORDANCIA'].apply(get_verbs).rename('verbs')\n",
    "    normd = batch['CONCORDANCIA'].apply(get_normd).rename('normalized')\n",
    "    ccomp = batch['CONCORDANCIA'].apply(has_ccomp).rename('has_ccomp')\n",
    "    dep = batch['CONCORDANCIA'].apply(get_dep).rename('dependencies')\n",
    "    pos = batch['CONCORDANCIA'].apply(get_pos).rename('pos')\n",
    "    details = batch['CONCORDANCIA'].apply(get_details).rename('details')\n",
    "    \n",
    "    batch = pd.concat([batch['id'], verbs, batch['CONCORDANCIA'], normd, ccomp, dep, pos, details], axis=1)\n",
    "    \n",
    "    utils.save_excel(file_path, batch, file_name+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5c17174",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = utils.get_save_path('p', 'corpes', is_test=True)\n",
    "file_name = (save_path/test_file).stem.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "941f6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "procd = [pd.concat([data.iloc[:, :-1], \n",
    "                    data.loc[:, 'CONCORDANCIA'].apply(nlp_es)], \n",
    "                    axis=1)]\n",
    "save_batch(procd, file_path=save_path, file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32997c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
